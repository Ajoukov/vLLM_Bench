# vLLM Bench

## Prereqs

* Kubernetes access via `~/.kube/config` (cluster/context already set).
* `kubectl` (with built-in kustomize).
* Python 3.10+
* Hugging Face token must have access to the chosen model (gated models will 403).

## Dependencies

The `init.sh` script will automatically create a virtual environment (`.venv`) and install the following Python dependencies:

* **requests** - HTTP client for API calls
* **datasets** - HuggingFace Datasets library for loading benchmark datasets
* **rouge-score** - ROUGE metric calculation for summarization tasks
* **tiktoken** - OpenAI tokenizer for token counting
* **huggingface_hub** - HuggingFace Hub client for downloading datasets
* **orjson** - Fast JSON serialization

If you need to install these manually (e.g., for development), run:

```bash
pip install requests datasets rouge-score tiktoken huggingface_hub orjson
```

## Files

```
.
├── bench.py                 # Benchmark runner over OpenAI/vLLM APIs
├── config.json              # Single source of truth (namespace, token, model, ports)
├── init.sh                  # Generates kustomize, deploys Job, waits, runs bench
├── k8s/                     # Rendered manifests (generated by init.sh)
├── template/                # Base manifests copied into k8s/ by init.sh
└── runs/                    # Outputs: *.jsonl and *_manifest.json
```

## Configure

Edit `config.json` (required keys shown):

```json
{
  "namespace": "usr-xxx-namespace",
  "hf_token": "hf_yyy",
  "port_local": 8080,
  "port_remote": 8000,
  "endpoint": "http://127.0.0.1:8080",
  "defaults": {
    "model": "facebook/opt-125m",
    "max_model_len": 35000,
    "api_kind": "completions",
    "max_tokens": 256,
    "temperature": 0.0,
    "top_p": 1.0,
    "qps": 2.0,
    "limit": 1,
    "stream": true,
    "seed": 1234,
    "out_dir": "./runs",
    "cache_dir": "./.hf-cache"
  },
  "workloads": { ... }
}
```

Notes:

* `hf_token` is injected as a Secret.
* `defaults.model` and `defaults.max_model_len` propagate to the vLLM container via ConfigMap.
* If using a gated model (e.g., Meta Llama), ensure the token’s account has accepted the license.

## Deploy + Run

```bash
./init.sh config.json
```

What it does:

* Generates `k8s/kustomization.yaml` (ConfigMap + Secret).
* Applies manifests (deletes existing Job if needed).
* Waits for the pod to be Ready.
* Port-forwards as per `port_local:port_remote`.
* Runs `bench.py run config.json`.

Outputs land in `./runs/`.

## Change the model or settings

* Edit `config.json`.
* Re-run:

  ```bash
  ./init.sh config.json
  ```

The script updates the ConfigMap/Secret and restarts the Job only if required (e.g., model changed or pod not Ready).

## Using `bench.py` directly

* List workloads:

  ```bash
  ./bench.py list
  ```
* Run all enabled:

  ```bash
  ./bench.py run config.json
  ```
* Run a category/workload:

  ```bash
  ./bench.py run config.json spsr alpaca
  ```

## Kubernetes manifests

* Edit base YAMLs under `template/` (e.g., probes, resources, Service).
* `init.sh` copies `template/*` into `k8s/` and injects values from `config.json`.

## Health / Access

* Health: `GET http://127.0.0.1:<port_local>/health`
* If you need manual port-forward via Service:

  ```bash
  kubectl -n <namespace> port-forward service/vllm-service 8080:80
  ```

