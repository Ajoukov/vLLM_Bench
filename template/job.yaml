apiVersion: batch/v1
kind: Job
metadata:
  name: vllm
  labels: { app: vllm }
spec:
  backoffLimit: 0
  template:
    metadata:
      labels: { app: vllm }
    spec:
      shareProcessNamespace: true
      restartPolicy: Never
      volumes:
        - name: app-volume
          persistentVolumeClaim: { claimName: vllm-pvc }
        - name: cache-volume
          persistentVolumeClaim: { claimName: vllm-pvc }
        - name: shm
          emptyDir: { medium: Memory, sizeLimit: "1G" }
      containers:
        - name: infinity
          image: ubuntu:22.04
          command: ["sleep","infinity"]
          env:
            - { name: VLLM_USE_FLASHINFER_SAMPLER, value: "0" }
            - { name: VLLM_LOGGING_LEVEL, value: "DEBUG" }
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: $(HF_SECRET_NAME)
                  key: token
            - { name: VLLM_ALLOW_DEPRECATED_BEAM_SEARCH, value: "1" }
            - { name: VLLM_ATTENTION_BACKEND, value: "TORCH_SDPA" }
        - name: vllm-container
          image: vllm/vllm-openai:v0.11.0
              # --host 0.0.0.0 --port 8000 --max-model-len 35000
          command: ["/bin/sh","-c"]
          args:
            - >
              vllm serve facebook/opt-125m
              --host 0.0.0.0 --port 8000 --max-model-len 1000
              --gpu-memory-utilization 0.3
          env:
            - { name: VLLM_USE_FLASHINFER_SAMPLER, value: "0" }
            - { name: VLLM_LOGGING_LEVEL, value: "DEBUG" }
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: $(HF_SECRET_NAME)
                  key: token
            - { name: VLLM_ALLOW_DEPRECATED_BEAM_SEARCH, value: "1" }
            - { name: VLLM_ATTENTION_BACKEND, value: "TORCH_SDPA" }

