# template/job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: vllm
  labels: { app: vllm }
spec:
  backoffLimit: 0
  template:
    metadata:
      labels: { app: vllm }
    spec:
      shareProcessNamespace: true
      restartPolicy: Never
      containers:
        - name: infinity
          image: ubuntu:22.04
          command: ["sleep","infinity"]
          envFrom:
            - configMapRef: { name: vllm-config }
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom: { secretKeyRef: { name: $(HF_SECRET_NAME), key: token } }
            - name: HF_TOKEN
              valueFrom: { secretKeyRef: { name: $(HF_SECRET_NAME), key: token } }

        - name: vllm-container
          image: vllm/vllm-openai:v0.11.0
          command: ["/bin/sh","-c"]
          args:
            - >
              vllm serve "$MODEL"
              --host 0.0.0.0
              --port 8000
              --max-model-len "$MAX_MODEL_LEN"
              --gpu-memory-utilization 0.3
          envFrom:
            - configMapRef: { name: vllm-config }
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom: { secretKeyRef: { name: $(HF_SECRET_NAME), key: token } }
            - name: HF_TOKEN
              valueFrom: { secretKeyRef: { name: $(HF_SECRET_NAME), key: token } }
          startupProbe:
            httpGet:
              path: /health
              port: 8000
            periodSeconds: 5
            failureThreshold: 120   # ~10 minutes
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 5
            periodSeconds: 5
            failureThreshold: 6
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 10
            failureThreshold: 3

